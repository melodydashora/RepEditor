{
  "generated_at": "2025-10-08T02:42:27.281Z",
  "research_date": "October 8, 2025",
  "providers": [
    {
      "provider": "OpenAI",
      "timestamp": "2025-10-08T02:41:21.862Z",
      "answer": "OpenAI’s current production flagship model for chatbot and completion tasks as of October 2025 is **gpt-5**, which introduces new API parameters and deprecates older models. Below are the verified details for production use:\n\n---\n\n### 1. Latest Model Names/IDs (API Call Strings)\n- **gpt-5** (exact string: `\"gpt-5\"`)\n- Older models like `gpt-4.5-preview` are deprecated and scheduled for removal[3].\n\n---\n\n### 2. API Endpoint URLs\n- **Completions/Chat:**  \n  `https://api.openai.com/v1/chat/completions`\n- **Assistants API (v2):**  \n  `https://api.openai.com/v2/assistants`\n- **AgentKit/Responses API:**  \n  (For agentic workflows, see AgentKit documentation)[4]\n\n---\n\n### 3. Required HTTP Headers\n- `Authorization: Bearer <YOUR_API_KEY>`\n- `Content-Type: application/json`\n- (Optional for organization): `OpenAI-Organization: <org-id>`\n\n---\n\n### 4. Supported Parameters (gpt-5)\n- **model**: `\"gpt-5\"`\n- **messages**: Array of message objects (for chat)\n- **reasoning_effort**: `\"minimal\"`, `\"low\"`, `\"medium\"`, `\"high\"`  \n  *(Replaces `temperature` and `top_p` for controlling response depth and creativity)*\n- **max_tokens**: Integer (maximum tokens in output)\n- **stop**: Array of stop sequences\n- **stream**: Boolean (for streaming responses)\n- **tools**: Tool/function calling (if enabled)\n- **user**: Optional user identifier\n\n*Note: `temperature` and `top_p` are not supported in gpt-5; use `reasoning_effort` instead.*\n\n---\n\n### 5. Context Window Sizes\n- **gpt-5**:  \n  *Context window is 256k tokens* (256,000 tokens per request, input + output combined)[1].\n\n---\n\n### 6. Pricing per Million Tokens (Input/Output)\n- **gpt-5** (as of October 2025, via Azure AI Foundry and OpenAI Platform)[1]:\n  - **Input:** $8.00 per million tokens\n  - **Output:** $24.00 per million tokens\n\n*Pricing may vary by region and platform (Azure/OpenAI direct).*\n\n---\n\n### 7. Special Features or Constraints\n- **reasoning_effort** parameter allows explicit control over model reasoning depth (unique to gpt-5).\n- **Multimodal support**: gpt-5 is integrated into Azure AI Foundry for text, image, and audio tasks, but the core API endpoint above is for text/chat[1].\n- **AgentKit**: New SDK and workflow tools for building agentic applications, including visual workflow design and multi-agent orchestration[4].\n- **Deprecations**: gpt-4.5-preview and earlier models are being phased out[3].\n- **Safety Upgrades**: Enhanced safety and content moderation features in gpt-5[1].\n\n---\n\n### 8. SDK Package Names\n- **Official OpenAI SDK:**  \n  - Python: `openai` (PyPI: `openai`)\n  - Node.js: `openai` (npm: `openai`)\n- **AgentKit:**  \n  - Node.js: `@openai/agentkit` (for agentic workflows)[4]\n- **Codex (code models):**  \n  - Node.js: `@openai/codex` (for code generation tasks)[6]\n\n---\n\n**Summary Table: OpenAI gpt-5 (October 2025)**\n\n| Feature                | Value/Details                                  |\n|------------------------|------------------------------------------------|\n| Model ID               | gpt-5                                          |\n| API Endpoint           | https://api.openai.com/v1/chat/completions     |\n| Required Headers       | Authorization, Content-Type                    |\n| Key Parameters         | model, messages, reasoning_effort, max_tokens  |\n| Context Window         | 256,000 tokens                                 |\n| Pricing (per 1M tokens)| $8 input / $24 output                          |\n| SDKs                   | openai, @openai/agentkit, @openai/codex        |\n| Special Features       | reasoning_effort, agentic workflows, multimodal|\n\n---\n\n**References:**  \n- Model and pricing: [1]  \n- Deprecations: [3]  \n- AgentKit/SDKs: [4][6]  \n\nIf you need details for Anthropic or Google models, please specify.",
      "citations": [
        "https://azure.microsoft.com/en-us/blog/unleash-your-creativity-at-scale-azure-ai-foundrys-multimodal-revolution/",
        "https://techcrunch.com/2025/10/03/what-to-expect-at-openais-devday-2025-and-how-to-watch-it/",
        "https://platform.openai.com/docs/deprecations",
        "https://openai.com/index/introducing-agentkit/",
        "https://cdn.openai.com/threat-intelligence-reports/7d662b68-952f-4dfd-a2f2-fe55b041cc4a/disrupting-malicious-uses-of-ai-october-2025.pdf",
        "https://openai.com/index/codex-now-generally-available/"
      ],
      "relatedQuestions": []
    },
    {
      "provider": "Anthropic",
      "timestamp": "2025-10-08T02:41:19.218Z",
      "answer": "**Anthropic’s current production AI model for chatbot/completion tasks as of October 2025 is Claude Sonnet 4.5, with model ID `claude-sonnet-4-5-20250929`[1][4][5].** Below is a detailed breakdown of its API usage and features, based strictly on official and authoritative sources from 2024–2025.\n\n---\n\n### 1. Latest Model Name/ID\n\n- **Model name:** Claude Sonnet 4.5\n- **Model ID (for API calls):** `claude-sonnet-4-5-20250929`[1][4][5]\n\n---\n\n### 2. API Endpoint URL\n\n- **Endpoint:**  \n  `https://api.anthropic.com/v1/messages`  \n  (This is the standard endpoint for Anthropic’s Claude models as of 2025)[5].\n\n---\n\n### 3. Required HTTP Headers\n\n- `x-api-key`: Your Anthropic API key\n- `anthropic-version`: Must be set to the current supported version, e.g., `2025-10-01`\n- `content-type`: `application/json`[5]\n\nExample:\n```http\nPOST /v1/messages\nx-api-key: YOUR_API_KEY\nanthropic-version: 2025-10-01\ncontent-type: application/json\n```\n\n---\n\n### 4. Supported Parameters\n\n- **model**: `\"claude-sonnet-4-5-20250929\"`\n- **max_tokens**: Maximum number of tokens to generate in the response\n- **temperature**: Controls randomness (range: 0.0–1.0)\n- **top_p**: Nucleus sampling (range: 0.0–1.0)\n- **system**: System prompt for model behavior\n- **messages**: Array of message objects (role: user/assistant, content)\n- **stop_sequences**: Array of strings to stop generation\n- **stream**: Boolean for streaming responses[5]\n\n---\n\n### 5. Context Window Size\n\n- **Context window:** Up to **200,000 tokens** for Claude Sonnet 4.5[5].\n- This enables handling very long conversations and documents.\n\n---\n\n### 6. Pricing per Million Tokens (Input/Output)\n\n- **Input tokens:** $8.00 per million tokens\n- **Output tokens:** $24.00 per million tokens  \n  (Pricing is based on Anthropic’s official transparency/model report as of October 2025)[5].\n\n---\n\n### 7. Special Features or Constraints\n\n- **Agentic AI:** Excels at multi-step reasoning and agentic tasks[1][4].\n- **Coding:** Best-in-class for software engineering, code generation, and debugging[1][4].\n- **Safety:** Enhanced safety and reliability features, suitable for enterprise and regulated industries[1][4].\n- **Long focus:** Can maintain context and focus for extended multi-hour tasks[1][4].\n- **Cybersecurity:** Used in advanced cybersecurity applications[3].\n- **Constraints:**  \n  - Maximum context window is 200,000 tokens.\n  - Rate limits and usage quotas apply per account (see Anthropic dashboard for details).\n\n---\n\n### 8. SDK Package Names\n\n- **Official SDK:**  \n  - Python: `anthropic`\n  - Node.js: `@anthropic-ai/sdk`  \n  (These packages are maintained by Anthropic and support Claude Sonnet 4.5)[5].\n\n---\n\n**Summary Table**\n\n| Feature                | Value/Details                                  |\n|------------------------|------------------------------------------------|\n| Model ID               | `claude-sonnet-4-5-20250929`                   |\n| API Endpoint           | `https://api.anthropic.com/v1/messages`        |\n| Required Headers       | `x-api-key`, `anthropic-version`, `content-type` |\n| Parameters             | `model`, `max_tokens`, `temperature`, `top_p`, `system`, `messages`, `stop_sequences`, `stream` |\n| Context Window         | 200,000 tokens                                 |\n| Pricing (Input/Output) | $8 / $24 per million tokens                    |\n| Special Features       | Agentic AI, coding, safety, long focus         |\n| SDK Packages           | `anthropic` (Python), `@anthropic-ai/sdk` (Node.js) |\n\n---\n\n**Claude Sonnet 4.5 (`claude-sonnet-4-5-20250929`) is Anthropic’s flagship production model for chatbot and completion tasks as of October 2025, offering industry-leading coding, agentic reasoning, and safety features with a 200k token context window and competitive pricing[1][4][5].**",
      "citations": [
        "https://officialmindai.com/university/top-5-ai-models-to-use-in-october-2025",
        "https://www.anthropic.com/news/deloitte-anthropic-partnership",
        "https://www.anthropic.com/research/building-ai-cyber-defenders",
        "https://www.anthropic.com/news/claude-sonnet-4-5",
        "https://www.anthropic.com/transparency/model-report"
      ],
      "relatedQuestions": []
    },
    {
      "provider": "Google AI (Gemini)",
      "timestamp": "2025-10-08T02:41:20.473Z",
      "answer": "As of October 2025, Google AI’s current production Gemini models for chatbot and completion tasks are led by **Gemini 2.5 Pro** and **Gemini 2.5 Flash**. Below is a detailed breakdown of each required specification, based strictly on official documentation and reliable sources from 2024–2025.\n\n---\n\n## 1. Latest Model Names/IDs (Exact Strings for API Calls)\n\n- **Gemini 2.5 Pro:**  \n  Model ID: `gemini-2.5-pro`[2][5]\n\n- **Gemini 2.5 Flash:**  \n  Model ID: `gemini-2.5-flash` (for text/image tasks; image editing is production-ready as of October 2025)[1][5]\n\n- **Gemini 2.5 Pro TTS:**  \n  Model ID: `gemini-2.5-pro-preview-tts` (for text-to-speech)[2][1]\n\n---\n\n## 2. API Endpoint URLs\n\n- **Vertex AI (Google Cloud):**\n  ```\n  POST https://us-central1-aiplatform.googleapis.com/v1/projects/{PROJECT_ID}/locations/{LOCATION}/publishers/google/models/{MODEL_ID}:predict\n  ```\n  - Replace `{MODEL_ID}` with `gemini-2.5-pro`, `gemini-2.5-flash`, or `gemini-2.5-pro-preview-tts`[2][4]\n\n- **Gemini API (Google AI for Developers):**\n  ```\n  POST https://generativelanguage.googleapis.com/v1beta/models/{MODEL_ID}:generateContent\n  ```\n  - Replace `{MODEL_ID}` as above[2]\n\n---\n\n## 3. Required HTTP Headers\n\n- `Authorization: Bearer {ACCESS_TOKEN}` (OAuth 2.0 or service account)[2][4]\n- `Content-Type: application/json`\n- For Gemini API:  \n  `X-Goog-Api-Key: {API_KEY}` (if using API key authentication)[2]\n\n---\n\n## 4. Supported Parameters\n\n- **temperature** (float, 0.0–1.0): Controls randomness[2]\n- **max_tokens** (integer): Maximum tokens in output[2]\n- **top_p** (float, 0.0–1.0): Nucleus sampling[2]\n- **stop_sequences** (array of strings): Stop generation on specified sequences[2]\n- **context** (string or array): Input context for conversation[2]\n- **system_instruction** (string): System prompt for role/context[2]\n- **safety_settings** (object): Content moderation[2]\n- **candidate_count** (integer): Number of completions to return[2]\n\n---\n\n## 5. Context Window Sizes\n\n| Model                | Context Window (Tokens) |\n|----------------------|------------------------|\n| gemini-2.5-pro       | **1,048,576** (1M tokens)[2] |\n| gemini-2.5-flash     | **65,536** tokens[2]         |\n| gemini-2.5-pro-preview-tts | **8,000–16,000** tokens[2] |\n\n---\n\n## 6. Pricing per Million Tokens (Input/Output)\n\n- **Gemini 2.5 Pro:**  \n  - Input: **$7.00 per million tokens**  \n  - Output: **$21.00 per million tokens**  \n  (Pricing as of June 2025; subject to change)[2][4]\n\n- **Gemini 2.5 Flash:**  \n  - Input: **$3.50 per million tokens**  \n  - Output: **$10.50 per million tokens**  \n  (Flash is optimized for speed and cost)[2][4]\n\n- **Gemini 2.5 Pro TTS:**  \n  - Pricing is per character or audio second; refer to Vertex AI pricing for latest rates[1]\n\n---\n\n## 7. Special Features or Constraints\n\n- **Gemini 2.5 Pro:**\n  - Multimodal: Supports text, code, images, audio, video, and PDF text[2]\n  - Advanced reasoning: Excels at code, math, STEM, and large document analysis[2][5]\n  - Long context: Up to 1M tokens for enterprise-scale tasks[2]\n  - Safety: Enterprise-grade content moderation and safety controls[2]\n  - Knowledge cutoff: January 2025[2]\n\n- **Gemini 2.5 Flash:**\n  - Optimized for speed and cost[5]\n  - Production-ready image generation and editing (Flash Image)[1]\n  - Batch processing and multi-aspect ratio support for images[1]\n\n- **Gemini 2.5 Pro TTS:**\n  - Studio-quality, multi-speaker dialogue in a single API call[1]\n  - Advanced style/tone/emotion control via natural language prompts[1]\n  - Supports 70+ languages[1]\n\n- **Constraints:**\n  - Some preview models only available in global region[4]\n  - Model updates may affect context window and pricing[2][4]\n\n---\n\n## 8. SDK Package Names\n\n- **Python:**  \n  - `google-cloud-aiplatform` (Vertex AI SDK)[2][4]\n  - `google-generativeai` (Gemini API SDK)[2]\n  - For Firebase: `firebase-ai-logic` (for Gemini integration)[4]\n\n---\n\n**Summary of flagship production model for chatbot/completion tasks:**\n\n- **Gemini 2.5 Pro** (`gemini-2.5-pro`) is Google’s latest and most advanced production model for reasoning, multimodal input, and long-context tasks, available via Vertex AI and Gemini API endpoints[2][5].\n- **Gemini 2.5 Flash** (`gemini-2.5-flash`) is optimized for speed and cost, suitable for high-throughput applications[1][5].\n- **Gemini 2.5 Pro TTS** (`gemini-2.5-pro-preview-tts`) is production-ready for advanced text-to-speech applications[1][2].\n\nAll information is sourced from official Google documentation and major tech news outlets from 2024–2025.",
      "citations": [
        "https://cloud.google.com/blog/products/ai-machine-learning/building-momentum-for-gen-media-including-nano-banana-",
        "https://ai.google.dev/gemini-api/docs/models",
        "https://www.eweek.com/news/gemini-flash-general-availability/",
        "https://firebase.google.com/docs/ai-logic/models",
        "https://openrouter.ai/google/gemini-2.5-flash-preview-09-2025",
        "https://en.wikipedia.org/wiki/Google_Gemini"
      ],
      "relatedQuestions": []
    },
    {
      "provider": "Perplexity AI",
      "timestamp": "2025-10-08T02:42:06.730Z",
      "answer": "Perplexity AI’s current production models as of October 2025 are primarily focused on **retrieval-augmented generation (RAG)** and answer synthesis, integrating both proprietary and third-party LLMs for chatbot and completion tasks. The company’s API and platform are designed for real-time, context-rich, and citation-backed answers, but Perplexity does not publicly market a standalone, branded LLM (like “Perplexity-3” or “Comet-2”) for direct API use. Instead, it offers a unified API that orchestrates multiple top-tier models and its own retrieval stack[1][2].\n\nBelow are the details based on the latest available information:\n\n---\n\n### 1. Latest Model Names/IDs (Exact Strings for API Calls)\n\n- **Perplexity’s own models:** No evidence of a public, standalone LLM model ID (e.g., “perplexity-llm-v3”) for direct API use as of October 2025. The API orchestrates retrieval and generation, often using third-party models[2].\n- **Third-party models available via Perplexity Pro API:**\n  - **OpenAI:** gpt-4o (no public evidence of gpt-5 via Perplexity as of October 2025)\n  - **Anthropic:** claude-3.5-sonnet (no public evidence of claude-sonnet-4-5-20250929 via Perplexity)\n  - **Google:** gemini-1.5-pro (no public evidence of gemini-2.5-pro via Perplexity)\n- **Image generation:** DALL-E 3, Stable Diffusion XL[2][6]\n\n**Note:** Perplexity’s own answer engine is not exposed as a raw LLM endpoint; it is accessed via the unified /answer or /search API[2].\n\n---\n\n### 2. API Endpoint URLs\n\n- **Primary endpoint:**  \n  ```\n  POST https://api.perplexity.ai/v1/answer\n  ```\n  or  \n  ```\n  POST https://api.perplexity.ai/v1/search\n  ```\n  (Endpoints may vary for image generation or advanced research features.)[2]\n\n---\n\n### 3. Required HTTP Headers\n\n- **Authorization:**  \n  ```\n  Authorization: Bearer <your_api_key>\n  ```\n- **Content-Type:**  \n  ```\n  Content-Type: application/json\n  ```\n- **Optional:**  \n  ```\n  X-Model: <model_id>\n  ```\n  (For specifying third-party models, if supported.)[2]\n\n---\n\n### 4. Supported Parameters\n\n- **prompt** (string): User query or instruction.\n- **model** (string): Model selection (e.g., \"gpt-4o\", \"claude-3.5-sonnet\").\n- **max_tokens** (integer): Maximum tokens in the response.\n- **temperature** (float): Controls randomness (if supported by the underlying model).\n- **top_p** (float): Nucleus sampling (if supported).\n- **sources** (boolean): Return citations.\n- **image_prompt** (string): For image generation tasks.\n- **deep_search** (boolean): Enables multi-step, in-depth research mode (“Pro Search”)[2].\n\n**Note:** Not all parameters are supported for every model. For example, OpenAI’s gpt-5 (if/when available) uses reasoning_effort instead of temperature/top_p, but this is not yet documented for Perplexity’s API[2].\n\n---\n\n### 5. Context Window Sizes\n\n- **Perplexity’s own RAG pipeline:** Not explicitly documented, but designed for large, multi-document context windows (often >32k tokens) due to hybrid retrieval[1].\n- **Third-party models:**  \n  - **gpt-4o:** Up to 128k tokens (if available)[2]\n  - **claude-3.5-sonnet:** Up to 200k tokens (if available)[2]\n  - **gemini-1.5-pro:** Up to 1M tokens (if available)[2]\n\n---\n\n### 6. Pricing per Million Tokens (Input/Output)\n\n- **Perplexity Pro API:**  \n  - $5 monthly credit included with Pro subscription[2]\n  - No public, granular per-token pricing for API usage as of October 2025. Pricing may depend on the underlying model and usage tier.\n- **Third-party models:**  \n  - Pricing is typically passthrough or slightly marked up from the provider (e.g., OpenAI, Anthropic, Google), but exact rates are not published by Perplexity[2].\n\n---\n\n### 7. Special Features or Constraints\n\n- **Retrieval-Augmented Generation:** All answers are grounded in real-time web search and cite sources[1][2].\n- **Deep Research (“Pro Search”):** Multi-step, multi-angle analysis for complex queries; takes longer but provides more comprehensive answers[2].\n- **Image Generation:** Integrated DALL-E 3 and Stable Diffusion XL for visual content[2][6].\n- **No raw LLM access:** Perplexity’s API is not a generic LLM completion endpoint; it is optimized for answer synthesis with citations[2].\n- **Premium Content:** “Comet Plus” subscription unlocks premium journalism sources for answers[7].\n\n---\n\n### 8. SDK Package Names\n\n- **No official SDKs** as of October 2025.  \n  - Developers use standard HTTP requests (e.g., via Python’s requests or JavaScript’s fetch).\n  - Community wrappers may exist, but none are officially maintained or documented by Perplexity[2].\n\n---\n\n#### Summary Table: Perplexity AI Production Model Access (October 2025)\n\n| Feature                | Perplexity AI (2025)                          |\n|------------------------|-----------------------------------------------|\n| Model IDs              | No public LLM ID; uses 3rd-party models       |\n| API Endpoint           | POST /v1/answer, /v1/search                   |\n| HTTP Headers           | Authorization, Content-Type, X-Model (opt.)   |\n| Parameters             | prompt, model, max_tokens, temperature, etc.  |\n| Context Window         | Large (multi-doc, >32k tokens typical)        |\n| Pricing                | $5/mo credit (Pro); no public per-token rate  |\n| Special Features       | RAG, citations, deep research, image gen      |\n| SDKs                   | None officially; use HTTP clients             |\n\n---\n\n**Flagship Model Verification:**  \n- **OpenAI gpt-5, Anthropic claude-sonnet-4-5-20250929, Google gemini-2.5-pro:**  \n  No evidence these specific latest models are available via Perplexity’s API as of October 2025. The most recent supported are gpt-4o, claude-3.5-sonnet, and gemini-1.5-pro[2].\n\n---\n\nIf you need details on Perplexity’s internal retrieval or ranking models, or want to know when support for the very latest LLMs is expected, monitoring their official API documentation and product blog is recommended[1][2].",
      "citations": [
        "https://joshuaberkowitz.us/blog/news-1/perplexity-is-redefining-search-apis-for-the-age-of-ai-1321",
        "https://skywork.ai/skypage/en/Perplexity-Pro:-The-Ultimate-Guide-to-the-AI-Answer-Engine/1974362683383410688",
        "https://firstpagesage.com/reports/top-generative-ai-chatbots/",
        "https://techcrunch.com/2025/10/02/perplexitys-comet-ai-browser-now-free-max-users-get-new-background-assistant/",
        "https://time.com/7323827/ai-browsers-perplexity-comet/",
        "https://opentools.ai/news/perplexity-ai-supercharges-creative-ai-with-visual-electric-acquisition",
        "https://www.storyboard18.com/digital/perplexity-ai-launches-comet-plus-aims-to-deliver-premium-journalism-in-ai-era-81932.htm",
        "https://www.opensourceforu.com/2025/10/perplexitys-free-ai-browser-comet-takes-aim-at-google-chrome-and-openai/"
      ],
      "relatedQuestions": []
    }
  ],
  "parameter_constraints": {
    "topic": "parameter_constraints",
    "timestamp": "2025-10-08T02:42:16.702Z",
    "answer": "**GPT-5 (OpenAI):**\n\n- **Does GPT-5 support temperature, top_p, frequency_penalty, or presence_penalty?**\n  - **No.** As of October 2025, **GPT-5 does not support** the following parameters:\n    - `temperature`\n    - `top_p`\n    - `frequency_penalty`\n    - `presence_penalty`\n  - These parameters are deprecated for GPT-5 and are not accepted by its API endpoint[1].\n\n- **What parameters does GPT-5's reasoning_effort replace?**\n  - The new parameter **`reasoning_effort`** replaces the legacy sampling and penalty parameters (`temperature`, `top_p`, `frequency_penalty`, `presence_penalty`). It controls the model's depth and thoroughness of reasoning instead of randomness or repetition penalties[1].\n\n- **What are the valid values for reasoning_effort?**\n  - Valid values for **`reasoning_effort`** are:\n    - `minimal`\n    - `low`\n    - `medium`\n    - `high`\n  - These are string values, not numeric ranges[1].\n\n- **Are there any breaking changes from GPT-4 to GPT-5?**\n  - **Yes.** The removal of `temperature`, `top_p`, `frequency_penalty`, and `presence_penalty` is a breaking change. Any code or integration relying on these parameters must be updated to use `reasoning_effort` instead[1].\n  - Additionally, the migration to the Responses API and deprecation of the Assistants API may affect some workflows[1][5].\n\n---\n\n**Claude Sonnet 4.5 (Anthropic):**\n\n- **Model ID:** The exact model ID is **`claude-sonnet-4-5-20250929`**.\n\n- **Supported parameters:**\n  - `model` (must be set to `claude-sonnet-4-5-20250929`)\n  - `messages` (array of role/content pairs)\n  - `max_tokens` (integer, typically up to 4096)\n  - `temperature` (float, usually 0.0–1.0)\n  - `top_p` (float, usually 0.0–1.0)\n  - `stop_sequences` (array of strings)\n  - `system` (optional system prompt)\n  - `tools` (function/tool calling, if supported)\n  - **No support for reasoning_effort.**\n\n---\n\n**Gemini 2.5 Pro (Google):**\n\n- **Supported parameters:**\n  - `model` (e.g., `gemini-2.5-pro`)\n  - `messages` (array of role/content pairs)\n  - `max_tokens` (integer, typically up to 8192)\n  - `temperature` (float, 0.0–2.0)\n  - `top_p` (float, 0.0–1.0)\n  - `stop` (array of strings)\n  - `tools` (function/tool calling, if supported)\n  - **No support for reasoning_effort.**\n\n---\n\n**Summary Table**\n\n| Model                    | Deprecated Parameters                | Supported Parameters (core)                                   | Special/Breaking Changes                |\n|--------------------------|--------------------------------------|---------------------------------------------------------------|-----------------------------------------|\n| **GPT-5**                | temperature, top_p, frequency_penalty, presence_penalty | model, messages, max_tokens, reasoning_effort, tools          | Must use reasoning_effort; legacy sampling/penalty params removed |\n| **Claude Sonnet 4.5**    | None                                 | model, messages, max_tokens, temperature, top_p, stop_sequences, system, tools | Model ID: claude-sonnet-4-5-20250929    |\n| **Gemini 2.5 Pro**       | None                                 | model, messages, max_tokens, temperature, top_p, stop, tools  | Standard OpenAI-style sampling params    |\n\n**Key Points:**\n- **GPT-5**: Only accepts `reasoning_effort` (`minimal`, `low`, `medium`, `high`). All sampling/penalty parameters are deprecated and will cause errors if used[1].\n- **Claude Sonnet 4.5**: Uses standard sampling parameters (`temperature`, `top_p`, etc.), no reasoning_effort.\n- **Gemini 2.5 Pro**: Uses standard sampling parameters (`temperature`, `top_p`, etc.), no reasoning_effort.\n\nIf you are migrating from GPT-4 to GPT-5, update your API calls to remove deprecated parameters and use `reasoning_effort` instead[1].",
    "citations": [
      "https://platform.openai.com/docs/deprecations",
      "https://apidog.com/blog/what-api-endpoints-available-codex-2025/",
      "https://gptforwork.com/help/release-notes",
      "https://learn.microsoft.com/en-us/azure/ai-foundry/openai/api-version-lifecycle",
      "https://platform.openai.com/docs/guides/migrate-to-responses",
      "https://skywork.ai/skypage/ko/GPT-OSS%20Playground:%20A%20Deep%20Dive%20into%20the%20New%20Era%20of%20AI%20Tools/1972578408011067392",
      "https://www.cursor-ide.com/blog/openai-apps-sdk",
      "https://help.openai.com/en/articles/10128477-chatgpt-enterprise-edu-release-notes%3F.xls",
      "https://www.datastudios.org/post/coding-with-chatgpt-context-sizes-tool-integrations-and-developer-workflows"
    ]
  },
  "deprecated_models": {
    "topic": "deprecated_models",
    "timestamp": "2025-10-08T02:42:27.281Z",
    "answer": "Several major AI models from OpenAI, Anthropic, and Google AI that were popular in 2024 have been officially deprecated or are scheduled for deprecation in late 2024 and early 2025. Below are the confirmed details for each provider, including model names, recommended replacements, and sunset dates.\n\n---\n\n**OpenAI**\n\n| Deprecated Model Name           | Sunset Date   | Recommended Replacement |\n|---------------------------------|--------------|------------------------|\n| gpt-4-32k, gpt-4-32k-0613, gpt-4-32k-0314 | 2025-06-06   | gpt-4o                 |\n| gpt-4-vision-preview, gpt-4-1106-vision-preview | 2024-12-06   | gpt-4o                 |\n| gpt-4.5-preview                 | 2025-07-14   | gpt-4.1                |\n| o1-preview                      | 2025-07-28   | o3                     |\n| o1-mini                         | 2025-10-27   | o4-mini                |\n| OpenAI-Beta: assistants=v1      | 2024-12-18   | OpenAI-Beta: assistants=v2 |\n| Codex (standalone API)          | 2023–2024    | GPT-3.5-Turbo, GPT-4, o3, o4-mini (for code/agentic tasks) |\n\n- **Sunset dates** are strictly enforced; after these dates, the models are no longer accessible via API[1][2].\n- **Codex** as a standalone API is no longer available; all code and agentic workflows have moved to newer chat-based models and agent endpoints[2].\n- **Assistants API v1** will be shut off by December 18, 2024; migration to v2 is required[1].\n\n---\n\n**Anthropic**\n\n- As of October 2025, there are no official public deprecation notices for Anthropic’s Claude models (Claude 2, Claude 3, etc.) in the search results provided.\n- Anthropic typically announces deprecations via their developer documentation and blog, but no major Claude model deprecations for late 2024 or early 2025 are confirmed in the available sources.\n\n---\n\n**Google AI**\n\n- No explicit deprecation announcements for Gemini (formerly Bard) or other major Google AI models in the search results provided.\n- Google typically phases out older models quietly as new Gemini versions are released, but no official sunset dates or model names are listed in the current results.\n\n---\n\n**Additional Notes**\n\n- **Azure OpenAI** mirrors OpenAI’s deprecation schedule for hosted models, with the same sunset dates for gpt-4-32k, gpt-4-vision-preview, and related endpoints[4].\n- **Open-source models** (e.g., DeepSeek, Llama) have seen rapid evolution, but no formal deprecation process is enforced as with closed models[3].\n\n---\n\n**Summary of Key Models to Avoid Using (OpenAI, 2024–2025):**\n- gpt-4-32k, gpt-4-32k-0613, gpt-4-32k-0314\n- gpt-4-vision-preview, gpt-4-1106-vision-preview\n- gpt-4.5-preview\n- o1-preview, o1-mini\n- OpenAI-Beta: assistants=v1\n- Codex (standalone API)\n\n**Recommended replacements** are gpt-4o, gpt-4.1, o3, o4-mini, and assistants=v2, depending on the use case[1][2].\n\nIf you need details on Anthropic or Google AI model deprecations, consult their official developer documentation, as no confirmed deprecations are listed in the current search results.",
    "citations": [
      "https://platform.openai.com/docs/deprecations",
      "https://skywork.ai/blog/openai-codex-definition-evolution-successors/",
      "https://www.aclu.org/news/privacy-technology/open-source-llms",
      "https://learn.microsoft.com/en-us/azure/ai-foundry/openai/concepts/legacy-models",
      "https://learn.microsoft.com/en-us/azure/ai-foundry/openai/api-version-lifecycle",
      "https://help.openai.com/en/articles/6825453-chatgpt-release-notes",
      "https://community.openai.com/t/just-informed-that-gpt-4o-mini-tts-is-about-to-be-deprecated/1361190"
    ]
  },
  "recommendations": [
    {
      "priority": "HIGH",
      "item": "GPT-5 Parameter Update",
      "detail": "GPT-5 uses reasoning_effort instead of temperature. Update server/lib/adapters/openai-gpt5.js to remove unsupported parameters."
    },
    {
      "priority": "HIGH",
      "item": "Model Deprecation",
      "detail": "Some models may be deprecated. Review the deprecated_models section and update your codebase."
    },
    {
      "priority": "MEDIUM",
      "item": "Claude 4.5 Verification",
      "detail": "Verify Claude Sonnet 4.5 model ID matches what you have in server/lib/adapters/anthropic-sonnet45.js"
    }
  ],
  "next_steps": [
    "Review all findings against official documentation",
    "Update server/lib/adapters/* files with new model names",
    "Update docs/reference/LLM_MODELS_REFERENCE.md",
    "Update .env.example with new model defaults",
    "Test each model with tools/testing endpoints",
    "Update ISSUES.md if deprecated models are still in use"
  ]
}